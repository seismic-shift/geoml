{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to neural networks: Regression\n",
    "\n",
    "This notebook is based on the SEG Geophysical Tutorial from August 2018 by Graham Ganssle: https://github.com/seg/tutorials-2018.\n",
    "\n",
    "The idea is to introduce the based components of an artificial neural network and implement a simple version of one using Numpy.\n",
    "\n",
    "We'll use a regression task — predicting a DT log from other logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is nothing but a nonlinear system of equations like $\\mathbf{y} = \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b})$.\n",
    "\n",
    "There are multiple functions $\\sigma$ that are used to introduce the non-linear component. One of the earliest was the *sigmoid* (aka *logistic*) function is given by:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + \\operatorname{e}^{-z}} $$\n",
    "\n",
    "Its derivative is:\n",
    "\n",
    "$$ \\frac{\\mathrm{d} \\sigma(z)}{\\mathrm{d} z} = \\sigma(z)  (1 - \\sigma(z)) $$\n",
    "\n",
    "We need the derivative for the _backpropagation_ process that enables neural networks to learn efficiently. Backpropagation adjusts the parameters of the neural network by injecting an error signal backwards through the network's layers, from the last to the first.\n",
    "\n",
    "We can implement the logistic function like this in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def logistic(z, derivative=False):\n",
    "    if not derivative:\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    else:\n",
    "        return z * (1 - z)  # In the implementation, 'z' will actually be sigma(z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function transforms, or 'squeezes', numbers into the range [0, 1] and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_activation\n",
    "\n",
    "plot_activation(logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, while this function is sometimes useful for handling probabilities, there are some problems with it.\n",
    "\n",
    "- The maximum value of the derivative is 0.25, which tends to reduce the learning rate, especially in deeper layers.\n",
    "- Large activations input result in 'saturation' and a gradient of 0 ('vanishing gradient'), which will halt learning.\n",
    "- The exponentials are expensive to compute.\n",
    "\n",
    "The $\\operatorname{tanh}$ function solves some of these issues — for example, it has a maximum gradient of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z, derivative=False):\n",
    "    \"\"\"\n",
    "    Compute a tanh transformation for a given input.\n",
    "    \"\"\"\n",
    "    if not derivative:\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    else:\n",
    "        return 1 - z**2  # In the implementation, we'll get tanh(z) coming at us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activation(tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it still suffers from the saturation issue, and the expense of computation.\n",
    "\n",
    "Both of these issues are solved by the ReLU, or rectified linear unit, function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "The **rectified linear unit** (ReLU) and its derivative are given by:\n",
    "\n",
    "$$ f(z) = \\begin{cases}\n",
    "    z & \\text{if } z > 0, \\\\\n",
    "    0 & \\text{otherwise}.\n",
    "\\end{cases} $$\n",
    "\n",
    "$$ \\frac{\\mathrm{d}f(z)}{\\mathrm{d}z} = \\begin{cases}\n",
    "    1 & \\text{if } z > 0, \\\\\n",
    "    0 & \\text{otherwise}.\n",
    "\\end{cases} $$\n",
    "\n",
    "The main problem with the ReLU is that, depending on how the weights are initialized, some units in the network might 'die' as they get into negative activations and never fire. Accordingly, a common variant of the ReLU is the 'parametric' ReLU, which has $f(z) = \\alpha z$, when $Z \\leq 0$ (the corresponding derivative is then just $\\alpha$). The parameter $\\alpha$ can be tuned like other hyperparameters. A typical value is 0.01.\n",
    "\n",
    "The parametric ReLU is also called a 'leaky' ReLU, but that term implies that the value of $\\alpha$ is not being considered as a hyperparameter or tuned in any way.\n",
    "\n",
    "Can you implement a ReLU? (Or, if you prefer, a parametric ReLU?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that if you use `if z > 0` in your code, then\n",
    "# the plot_activation function won't work, because it\n",
    "# defines z as an array to make its plots. In general,\n",
    "# it's a good idea to write functions that work for\n",
    "# both scalars and arrays, where possible.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# ReLU solution.\n",
    "def relu(z, derivative=True):\n",
    "    \"\"\"\n",
    "    Compute a Rectified Linear Unit transformation for a given input.\n",
    "    \"\"\"\n",
    "    if not derivative:\n",
    "        return z * (z > 0)\n",
    "    else:\n",
    "        return 1 * (z > 0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# ALTERNATIVE\n",
    "# Parametric ReLU and a ReLU that builds off it.\n",
    "def prelu(z, derivative=False, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Compute a Parameter Rectified Linear Unit transformation for a given input.\n",
    "    \"\"\"\n",
    "    if not derivative:\n",
    "        return np.maximum(alpha * z, z)  # Assumes alpha <= 1.\n",
    "    else:\n",
    "        return alpha * (z <= 0) + (z > 0)\n",
    "    \n",
    "def relu(z, derivative=False):\n",
    "    \"\"\"\n",
    "    An ordinary ReLU.\n",
    "    \"\"\"\n",
    "    return prelu(z, alpha=0, derivative=derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (relu(-1), relu(0), relu(1)) == (0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_activation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73d4b58f211c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_activation' is not defined"
     ]
    }
   ],
   "source": [
    "plot_activation(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### Stretch exercise\n",
    "\n",
    "Some people prefer the exponential linear unit, because it has a smooth derivative. Can you implement it?\n",
    "\n",
    "$$ f(z) = \\begin{cases} z & \\text{if } z > 0 \\\\  \\alpha(e^z-1) & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "The derivative is given by:\n",
    "\n",
    "$$ \\frac{\\mathrm{d} f}{\\mathrm{d} z} = \\begin{cases} 1 & \\text{if } z > 0 \\\\  \\alpha e^z & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "Again, $\\alpha$ is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the [Intro_to_neural_network_regression.ipynb](../master/Intro_to_neural_network_regression.ipynb) master notebook for a solution to this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def elu(z, derivative=False, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Compute an Exponential Linear Unit transformation for a given input.\n",
    "    \"\"\"\n",
    "    if not derivative:\n",
    "        return z * (z > 0) + alpha * (np.exp(z) - 1) * (z <= 0)\n",
    "    else:\n",
    "        return 1 * (z > 0) + alpha * np.exp(z) * (z <= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "plot_activation(elu, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still other rectifiers — e.g. the GELU and SiLU — read about them [on Wikipedia](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)). Why not try implementing some of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "We're going to need a way to tell when we're doing well. The **loss function** is some measure of error. We'll use the mean squared error, where the error is the difference between a known value of the target and our estimate of the target.\n",
    "\n",
    "We're going to need a function for that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Compute half the mean squared error. The factor of 0.5 gets cancelled by the\n",
    "    squared term in the derivative, so it's common to see it in the loss function.\n",
    "    \"\"\"\n",
    "    return 0.5 * np.mean((np.array(y_hat) - np.array(y))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical neural network consist of three or more *layers*: an input layer, one or more _hidden_ layers, and an output layer.\n",
    "\n",
    "Let's implement a network with one hidden layer. The layers are as follows:\n",
    "\n",
    "$$ \\text{Input layer:}\\ \\ \\mathbf{x}^{(i)} $$\n",
    "\n",
    "$$ \\text{Hidden layer:}\\ \\ \\mathbf{a}_1^{(i)} = \\sigma ( \\mathbf{W}_1 \\mathbf{x}^{(i)} + \\mathbf{b}_1) $$\n",
    "\n",
    "$$ \\text{Output layer:}\\ \\ \\hat{\\mathbf{y}}^{(i)} = \\mathbf{W}_2 \\mathbf{a}_1^{(i)} + \\mathbf{b}_2 $$\n",
    "\n",
    "where $\\mathbf{x}^{(i)}$ is the $i$-th sample of the input data $\\mathbf{X}$. $\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2$ are the weight matrices and bias vectors for layers 1 and 2 respectively, and $\\sigma$ is our nonlinear function. Applying the nonlinearity to $\\mathbf{W}_1 \\mathbf{x}^{(i)} + \\mathbf{b}_1$ in layer 1 results in the _activation_ $\\mathbf{a}_1$. The output layer yields $\\hat{\\mathbf{y}}^{(i)}$, the $i$-th estimate of the desired output. We're not going to apply the nonlinearity to the output, but people often do. The weights are randomly initialized and the biases start at zero; during training they will be iteratively updated to encourage the network to converge on an optimal approximation to the expected output.\n",
    "\n",
    "\n",
    "Note that these are vector operations. In `Numpy` we can easily deal with this because the library understands proper matrix operations. For example, matrix multiplication is done through the `@` operator.\n",
    "\n",
    "A forward pass of the data through the network looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def forward(xi, W1, b1, W2, b2, activation):\n",
    "    z1 = W1 @ xi + b1\n",
    "    a1 = activation(z1)\n",
    "    z2 = W2 @ a1 + b2  # n.b. z2 is y_hat\n",
    "    return z2, a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a picture of a neural network similar to the one we're building:\n",
    "\n",
    "![image](../images/figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a neural net learn?\n",
    "\n",
    "The short version is that we show the system a bunch of corresponding input/output pairs we want it to learn, and we show it these pairs thousands of times. Every time we do so, we move the **W**'s and **b**'s in whatever direction makes the outputs of the network more similar to the known output we're trying to teach it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "    For each training example:\n",
    "        For each layer:\n",
    "          - Calculate the error.\n",
    "          - Calculate weight gradient.\n",
    "          - Update weights.\n",
    "          - Calculate the bias gradient.\n",
    "          - Update biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's all this about gradients?\n",
    "\n",
    "In order to learn, the network will have to find the parameters (weights and biases) that result in the smallest loss. We'll use gradient descent for this. \n",
    "\n",
    "<img src=\"../images/gradient_descent.png\" width=\"800px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is straightforward for the output layer. That's why we needed the derivative in the activation functions, and we need to know the derivative for the `loss()` function.\n",
    "\n",
    "The error on the output layer for a given instance (data record) looks like this:\n",
    "\n",
    "$$ E = \\frac{1}{2} \\left[ \\hat{y}^{(i)} - y^{(i)} \\right]^2 $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\hat{y}^{(i)} = \\mathbf{w}_2 \\mathbf{a}_1^{(i)} + b_2 $$\n",
    "\n",
    "The derivative (gradient, or slope) of this function, with respect to the weight **w**<sub>2</sub>, is:\n",
    "\n",
    "$$ \\frac{\\mathrm{d}E}{\\mathrm{d}\\mathbf{w_2}} =  \\frac{\\mathrm{d}E}{\\mathrm{d}\\hat{y}}\\frac{\\mathrm{d}\\hat{y}}{\\mathrm{d}\\mathbf{w_2}} = (\\hat{y} - y) \\ \\mathbf{a}_1$$\n",
    "\n",
    "To calculate the gradient at the hidden layer, we need to compute the gradient of the error with respect to the weights and biases of the hidden layer.\n",
    "\n",
    "Let's implement this as a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def backward(xi, yi,\n",
    "             a1, z2,\n",
    "             params,\n",
    "             learning_rate,\n",
    "             activation\n",
    "            ):\n",
    "\n",
    "    err_output = z2 - yi  # Derivative of loss function\n",
    "    grad_W2 = err_output * a1\n",
    "    params['W2'] -= learning_rate * grad_W2\n",
    "\n",
    "    grad_b2 = err_output\n",
    "    params['b2'] -= learning_rate * grad_b2\n",
    "\n",
    "    derivative = activation(a1, derivative=True)\n",
    "    err_hidden = err_output * derivative * params['W2']\n",
    "    grad_W1 = err_hidden[:, None] @ xi[None, :]\n",
    "    params['W1'] -= learning_rate * grad_W1\n",
    "    \n",
    "    grad_b1 = err_hidden\n",
    "    params['b1'] -= learning_rate * grad_b1\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick with the `None` indexing is the same as reshaping the array. We have to do this to produce a 2D array for the `W1` gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate this backpropagation workflow, and thus that our system can learn, let's try to get the above neural network to learn the relationship between a DT log and some other logs. We're going to need some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import welly\n",
    "\n",
    "w = welly.Well.from_las('../data/R-90.las', index='original')\n",
    "\n",
    "data = w.data_as_matrix(keys=['GR', 'NPHISS', 'RHOB', 'DT'], start=1000, stop=3500, step=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_val = data[6500:6750, :3].reshape(-1, 3)\n",
    "X_train = data[6750:7750, :3].reshape(-1, 3)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "ax0.plot(X_train)\n",
    "ax1.plot(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.displot(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many situations, we do not need to scale the target variable. But when using gradient descent for optimization — essentially in all neural nets — we might need to worry about it. \n",
    "\n",
    "Very large errors may lead to exploding gradients in training and/or result in floating point overflows — especially if you're using GPUs, which use single-precision floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_ = data[6500:6750, -1]    # Keep the unscaled data.\n",
    "y_train_ = data[6750:7750, -1]\n",
    "\n",
    "target_scaler = StandardScaler().fit(y_train_.reshape(-1, 1))\n",
    "\n",
    "y_train = target_scaler.transform(y_train_.reshape(-1, 1))\n",
    "y_val = target_scaler.transform(y_val_.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "ax0.plot(y_train)\n",
    "ax1.plot(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the weights and biases for our network. A common approach is to initialize the weights with small random numbers (with NumPy's `randn()` function) and the biases with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "Finish the `initialize_params()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_params(features, units, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    params = {\n",
    "        \"W1\": np.random.randn(units, features),\n",
    "        \"b1\": np.zeros(shape=units),\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Initialize W2 (shape is just `units`) and b2 (shape is `1`)\n",
    "        \n",
    "        \n",
    "        # ===============\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_params(features, units, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    params = {\n",
    "        \"W1\": np.random.randn(units, features),\n",
    "        \"b1\": np.zeros(shape=units),\n",
    "\n",
    "        \"W2\": np.random.randn(units),\n",
    "        \"b2\": np.zeros(shape=1)\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.shape[-1]\n",
    "units = 5  # Units in hidden layer.\n",
    "\n",
    "params = initialize_params(features, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a network! It just doesn't know anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "To apply this (untrained) network to some data, we're going to need a `predict` function, to make inferences from the trained network. This mode of application is called **inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "Finish the `predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def predict(X, forward, params, activation):\n",
    "    \"\"\"\n",
    "    Make a prediction for a given 2D input ``X``,\n",
    "    using function ``forward``.\n",
    "    \"\"\"\n",
    "    y_hats = []\n",
    "    for xi in X:\n",
    "        # YOUR CODE HERE\n",
    "        # You need to call `forward` to set a value for `y_hat`.\n",
    "        \n",
    "        # ==============\n",
    "        y_hats.append(y_hat.item())\n",
    "    return np.array(y_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def predict(X, forward, params, activation):\n",
    "    \"\"\"\n",
    "    Make a prediction for a given 2D input ``X``,\n",
    "    using function ``forward``.\n",
    "    \"\"\"\n",
    "    y_hats = []\n",
    "    for xi in X:\n",
    "        y_hat, _ = forward(xi, **params, activation=activation)\n",
    "        y_hats.append(y_hat.item())\n",
    "    return np.array(y_hats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a prediction for our untrained network — it should be essentially random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_train, forward, params, activation=relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_train[:200])\n",
    "plt.plot(y_pred[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "During training, we expose the network to the input/output pairs one at a time. These pairs are called `xi` and `yi` respectively in the code. According to our diagram above, the input goes into the green slots and we adjust the orange neurons to make the red slot output from the network a tiny bit closer to the true DT result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this many times. Every time we do, we calculate the mean squared error between the network's prediction and the ground-truth output. After many iterations, or *epochs*, we draw a plot which shows the total error, or loss, at each step. If the network is learning anything, we expect the loss to decrease, as the predictions are getting closer to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "activation = elu\n",
    "\n",
    "# Intitialize.\n",
    "data = list(zip(X_train, y_train, y_train_))\n",
    "params = initialize_params(features, units)\n",
    "loss_history = []\n",
    "\n",
    "for i in tqdm(range(num_epochs)):\n",
    "\n",
    "    # Shuffle and prepare.\n",
    "    np.random.shuffle(data)\n",
    "    y_, y_hat = [], []\n",
    "    \n",
    "    for xi, yi, y_raw in data:\n",
    "        \n",
    "        # Optionally do a pass for validation (omitted here).\n",
    "        \n",
    "        # Forward pass.\n",
    "        z2, a1 = forward(xi, **params, activation=activation)\n",
    "\n",
    "        # Back propagation.\n",
    "        params = backward(xi, yi,\n",
    "                          a1, z2.item(),\n",
    "                          params,\n",
    "                          learning_rate,\n",
    "                          activation=activation\n",
    "                         )\n",
    "        \n",
    "        # Capture actual prediction at correct scale.\n",
    "        y_.append(y_raw)\n",
    "        y_hat.append(target_scaler.inverse_transform(z2))\n",
    "\n",
    "    # Compute training loss for this epoch.\n",
    "    loss_history.append(loss(y_, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the model are now no longer random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They do look kind of random though. It's usually hard to 'see' what neural networks have learned. Let's look at the W1 weights only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = params['W1']\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "_ = plt.imshow(W1.T, aspect='auto', vmin=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the network learned anything **useful** then the loss should have decreased during training. The loss is our measure of whatever it is we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "\n",
    "ax.semilogy(loss_history, label='Training loss')\n",
    "\n",
    "ax.set_title('Mean squared error vs epoch number', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_val, forward, params, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreased dramatically over the course of relatively few epochs, so presumably the network has learned something. To test this theory, let's plot the outputs after training (orange) and compare them to the expected result (blue):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(y_val)\n",
    "plt.plot(y_pred)\n",
    "plt.grid(c='k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare using RMS error\n",
    "\n",
    "It's fine for the network to learn using MSE, but it's easier for humans to understand RMS error, because it has the same units as the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "Implement an equation for the RMS error.\n",
    "\n",
    "$$ E_\\mathrm{RMS} = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N} (\\hat{y} - y)^2 } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the square root of the mean squared error.\n",
    "    \"\"\"\n",
    "    mse = np.sum((y_pred - y_true)**2) / y_true.size\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_val_, target_scaler.inverse_transform(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "## Exercise: how does this network look in `scikit-learn`?\n",
    "\n",
    "Replicate this neural network with `sklearn.neural_network.MLPRegressor`.\n",
    "\n",
    "You will have to read the documentation carefully. In particular, pay attention to `solver`, `activation`, `max_iter`, and `batch_size`.\n",
    "\n",
    "Get started with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(5,),\n",
    "                   tol=1e-12,   # Turn off early stopping.\n",
    "                   momentum=0,  # Turn off momentum                   \n",
    "                   \n",
    "                   # YOUR CODE HERE\n",
    "                   \n",
    "                  )\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred_skl = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(y_val_)\n",
    "plt.plot(target_scaler.inverse_transform(y_pred))\n",
    "plt.plot(target_scaler.inverse_transform(y_pred_skl))\n",
    "plt.grid(c='k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(5,),\n",
    "                   activation='relu',\n",
    "                   alpha=0,       # No regularization.\n",
    "                   solver='sgd',  # Default is 'adam'.\n",
    "                   learning_rate_init=0.001,   # Default.\n",
    "                   learning_rate='constant',   # Default.\n",
    "                   batch_size=1,  # Default is min(200, data length)\n",
    "                   max_iter=100,\n",
    "                   momentum=0,    # Turn off momentum\n",
    "                  )\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred_skl = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(y_val_)\n",
    "plt.plot(target_scaler.inverse_transform(y_pred))\n",
    "plt.plot(target_scaler.inverse_transform(y_pred_skl))\n",
    "plt.grid(c='k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scratch NN\")\n",
    "print(rmse(y_val_, target_scaler.inverse_transform(y_pred)))\n",
    "print()\n",
    "print(\"Sklearn NN\")\n",
    "print(rmse(y_val_, target_scaler.inverse_transform(y_pred_skl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "Can you change the hyperparameters to get a better result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the solution from the last example here.\n",
    "# Then change some of the parameters and see how it affects the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(20,),\n",
    "                   activation='relu',\n",
    "                   alpha=0.001,\n",
    "                   solver='adam',\n",
    "                   learning_rate_init=0.001,\n",
    "                   learning_rate='constant',\n",
    "                   batch_size=200,\n",
    "                   max_iter=1000,\n",
    "                   momentum=0.9,\n",
    "                  )\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred_skl = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(y_val_)\n",
    "plt.plot(target_scaler.inverse_transform(y_pred))\n",
    "plt.plot(target_scaler.inverse_transform(y_pred_skl))\n",
    "plt.grid(c='k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scratch NN\")\n",
    "print(rmse(y_val_, target_scaler.inverse_transform(y_pred)))\n",
    "print()\n",
    "print(\"Sklearn NN\")\n",
    "print(rmse(y_val_, target_scaler.inverse_transform(y_pred_skl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pt = torch.tensor(X_train, dtype=torch.float32).to(device) \n",
    "y_train_pt = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "traindata = torch.utils.data.TensorDataset(X_train_pt, y_train_pt)\n",
    "trainloader = torch.utils.data.DataLoader(traindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a high-level approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(3, 5),\n",
    "    nn.ELU(), \n",
    "    nn.Linear(5, 1),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a low-level approach that gives you fine-tuned control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(3, 5)  # aka \"Fully-connected\"\n",
    "        self.output = nn.Linear(5, 1)\n",
    "\n",
    "        # Optional.\n",
    "        nn.init.xavier_uniform_(self.hidden.weight)\n",
    "        nn.init.zeros_(self.hidden.bias)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        nn.init.zeros_(self.output.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = self.hidden(x)\n",
    "        a1 = torch.nn.functional.elu(z1)\n",
    "        z2 = self.output(a1)\n",
    "        return z2\n",
    "    \n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "weight_decay = 0.0  # L2 regularization\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "net.train()\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        xi, yi = data\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = net(xi)\n",
    "        loss = criterion(y_hat, yi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"# {epoch+1}  Loss {epoch_loss}\")\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_pt = torch.tensor(X_val, dtype=torch.float).to(device)\n",
    "y_val_pt = torch.tensor(y_val.reshape(-1, 1), dtype=torch.float).to(device)\n",
    "\n",
    "valdata = torch.utils.data.TensorDataset(X_val_pt, y_val_pt)\n",
    "valloader = torch.utils.data.DataLoader(valdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_torch = [float(net(xi)) for xi, yi in valloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(y_val_)\n",
    "plt.plot(target_scaler.inverse_transform(y_pred))\n",
    "plt.plot(target_scaler.inverse_transform(y_pred_skl))\n",
    "plt.plot(target_scaler.inverse_transform(y_pred_torch))\n",
    "plt.grid(c='k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scratch NN\")\n",
    "print(rmse(y_val_, target_scaler.inverse_transform(y_pred)))\n",
    "print()\n",
    "print(\"Sklearn NN\")\n",
    "print(rmse(y_val_, target_scaler.inverse_transform(y_pred_skl)))\n",
    "print()\n",
    "print(\"PyTorch NN\")\n",
    "print(rmse(y_val_, target_scaler.inverse_transform(y_pred_torch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a PyTorch model\n",
    "\n",
    "It is possible to save the mode with `torch.save(model, PATH)`, but this is not recommended because it depends on the exact structure of the project (files, directories, etc). Instead, PyTorch docs recommend saving the model class \n",
    "\n",
    "We can save the model's parameters to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"dt_model.pth\"\n",
    "torch.save(net.state_dict(), fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and read them into a new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_net = Net()\n",
    "saved_net.load_state_dict(torch.load(fname))\n",
    "\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_torch_ = [float(saved_net(xi)) for xi, yi in valloader]\n",
    "\n",
    "# Check it's the same as before.\n",
    "np.all(y_pred_torch == y_pred_torch_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "### EXERCISE\n",
    "\n",
    "Make a prediction using `sklearn.linear_model.Ridge`. How does it compare to the neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# End with...\n",
    "y_pred_linreg = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "regr = Ridge()\n",
    "\n",
    "regr.fit(X_train, y_train_)  # Don't need to scale y.\n",
    "\n",
    "y_pred_linreg = regr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_val_)\n",
    "plt.plot(target_scaler.inverse_transform(y_pred))\n",
    "plt.plot(target_scaler.inverse_transform(y_pred_skl))\n",
    "plt.plot(target_scaler.inverse_transform(y_pred_torch))\n",
    "plt.plot(y_pred_linreg)\n",
    "plt.grid(c='k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scratch NN\")\n",
    "print(rmse(y_val_, target_scaler.inverse_transform(y_pred)))\n",
    "print()\n",
    "print(\"Sklearn NN\")\n",
    "print(rmse(y_val_, target_scaler.inverse_transform(y_pred_skl)))\n",
    "print()\n",
    "print(\"PyTorch NN\")\n",
    "print(rmse(y_val_, target_scaler.inverse_transform(y_pred_torch)))\n",
    "print()\n",
    "print(\"Linear regression\")\n",
    "print(rmse(y_val_, y_pred_linreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exe"
    ]
   },
   "source": [
    "## Optional exercises\n",
    "\n",
    "Try to do these exercises on the NumPy implementation. But if that proves too difficult, use the `sklearn` implementation.\n",
    "\n",
    "- Try changing the model parameters, for example using fewer units in the hidden layer. Does this help?\n",
    "- Add another layer to the model. Does this help?\n",
    "- Try using other activation functions than the logistic function we're currently using.\n",
    "- Implement batches, RMSprop, and momentum.\n",
    "\n",
    "### Stretch\n",
    "\n",
    "If you've taken the Mastery class, or know about object oriented programming, write a Python `class` to hold the NumPy implementation. Copy the `keras`/`sklearn` interface as closely as possible. Related: [this awesome video from Joel Grus](https://www.youtube.com/watch?v=o64FV-ez6Gw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other types of neural networks\n",
    "\n",
    "![image](../images/nn_arch_cheatsheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "&copy; 2021 Agile Scientific and Graham Ganssle — Content is CC-BY-SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "geoml",
   "language": "python",
   "name": "geoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
