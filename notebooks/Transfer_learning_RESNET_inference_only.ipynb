{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Fossilnet inference\n", "\n", "We trained **fossilnet** on Google Colab's CPU-accelerated notebook, and saved the model weights in `./geofignet.pt`.\n", "\n", "You will need to install [PyTorch](https://pytorch.org/get-started/locally/) version >= 1.4."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torchvision\n", "\n", "torch.__version__, torchvision.__version__"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch.nn as nn\n", "from torchvision import datasets, models, transforms"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Instantiate the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class_names = ['ammonites',\n", "               'bivalves',\n", "               'corals',\n", "               'dinosaurs',\n", "               'echinoderms',\n", "               'fishes',\n", "               'forams',\n", "               'gastropods',\n", "               'plants',\n", "               'trilobites',\n", "              ]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You will need to download the weights from your Google Drive, or you can [use this one](https://drive.google.com/open?id=1Uf7TCzje8sfSrC_mVYjKAjpnHUUuDKdf)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Instantiate a vanilla ResNet and adjust its shape.\n", "model = models.resnet18()\n", "model.fc = nn.Linear(model.fc.in_features, len(class_names))\n", "\n", "# Load the geofignet weights.\n", "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n", "model.load_state_dict(torch.load('../data/fossilnet.pt', map_location=device), strict=False)\n", "\n", "# Set the mode to 'evaluate' before inference, e.g. to disable dropout layers.\n", "_ = model.eval()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Inference on one image"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from IPython.display import Image as Img\n", "\n", "Img(\"../data/random_ammonite.jpeg\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from PIL import Image\n", "\n", "data_transforms = transforms.Compose([\n", "        transforms.Resize(156),\n", "        transforms.CenterCrop(128),\n", "        transforms.ToTensor(),\n", "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n", "    ])\n", "\n", "def image_loader(image_name):\n", "    \"\"\"load image, returns cuda tensor\"\"\"\n", "    image = Image.open(image_name).convert('RGB')\n", "    image = data_transforms(image).unsqueeze(0)\n", "    return image.to(device)\n", "\n", "image = image_loader(\"../data/random_ammonite.jpeg\")\n", "\n", "sm = torch.nn.Softmax(dim=1)\n", "probs = sm(model(image))\n", "prob, clas = torch.max(probs, 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class_names[clas]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We also get the probability of the class selection:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["prob.item()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This came from the model output, which is passed through a softmax function:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["torch.nn.Softmax(dim=1)(model(image))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is a torch tensor, which was can convert to a NumPy object for easier manipulation:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" probs.detach().numpy().squeeze()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For example, we could make a plot of the log probability of each class:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["probs_ = probs.detach().numpy().squeeze()\n", "y = np.arange(len(probs_))\n", "y_min, y_max = y[0]-0.75, y[-1]+0.75\n", "\n", "fig, ax = plt.subplots(figsize=(6, 10))\n", "bars = ax.barh(y, probs_, color='orange', align='center', lw=2)\n", "ax.set_yticks(y)\n", "ax.set_yticklabels(class_names, size=14)\n", "ax.set_xscale('log')\n", "ax.set_ylim(y_max, y_min)  # Label top-down.\n", "ax.grid(c='black', alpha=0.1, which='both')\n", "\n", "for i, p in enumerate(probs_):\n", "    ax.text(0.55*min(probs_), i, f\"{p:0.2e}\", va='center')\n", "\n", "bars[np.argmax(probs_)].set_color('red')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## What next?\n", "\n", "If we think our model is doing what we want, we could deploy it in a web app for example. \n", "\n", "See an implementation of `geofignet` here:\n", "\n", "> https://geofignet.geosci.ai/\n", "\n", "But before we feel too pleased with ourselves:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Img(\"../data/cinnamon.jpg\", width=512)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["image = image_loader(\"../data/cinnamon.jpg\")\n", "\n", "sm = torch.nn.Softmax(dim=1)\n", "probs = sm(model(image))\n", "prob, clas = torch.max(probs, 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class_names[clas], prob.item()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "geocomp-ml", "language": "python", "name": "geocomp-ml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.6"}}, "nbformat": 4, "nbformat_minor": 2}