{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Image classification with `skorch`\n", "\n", "**Make sure you look at [`Intro to image classification`](Intro_to_image_classification.ipynb) before coming here.**\n", "\n", "**I recommend using the notebook [`Image classification with PyTorch`](Image_classification_with_Scikit-Learn.ipynb) instead of this one.**\n", "\n", "We'll use `Pytorch` together with supporting libraries `tensorlayers` and `skorch` to train a classifier for fossil images."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## The fossil dataset\n", "\n", "Let's generate a workflow to classify images using a CNN.\n", "We'll make use of a collection of functions in `utils.py` to help process the images found in the `data/fossils` folder."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = np.load('../data/fossils/X.npy')\n", "y = np.load('../data/fossils/y.npy')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.imshow(X_train[1].reshape(32,32))\n", "plt.colorbar()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## A convolutional neural network with skorch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n", "\n", "encoder = LabelEncoder()\n", "encoder.fit(np.append(y_train, y_val))\n", "\n", "y_train = encoder.transform(y_train)\n", "y_val = encoder.transform(y_val)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Skorch can use NumPY arrays, which is nice, but for some reason they have to be single precision (i.e. 32-bit floats)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train = X_train.reshape(-1, 1, 32, 32).astype(np.float32)\n", "X_val = X_val.reshape(-1, 1, 32, 32).astype(np.float32)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "from torch import nn\n", "import torchlayers as tl\n", "\n", "# torch.nn and torchlayers can be mixed easily.\n", "net_arch = torch.nn.Sequential(\n", "    tl.Conv(32, kernel_size=3),  # specify ONLY out_channels\n", "    nn.ELU(),  # use torch.nn wherever you wish\n", "    tl.BatchNorm(),\n", "    tl.Conv(16, kernel_size=3),\n", "    nn.ELU(),  \n", "    tl.BatchNorm(),\n", "    tl.GlobalMaxPool(),\n", "    tl.Linear(100), # Add a fully connected hidden layer\n", "    tl.ELU(), # Activate the hidden layer\n", "    tl.Linear(3),  # Output for 3 classes\n", "    tl.Softmax(dim=-1)\n", ")\n", "\n", "net = tl.build(net_arch, torch.randn(1, *X_train[0].shape))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["net"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from skorch import NeuralNetClassifier\n", "from skorch.callbacks import EarlyStopping, Checkpoint\n", "\n", "# Callbacks.\n", "cp = Checkpoint(dirname='skorch_cp')\n", "es = EarlyStopping(monitor='valid_loss', patience=9)\n", "\n", "cnn = NeuralNetClassifier(\n", "    net,\n", "    max_epochs=100,\n", "    batch_size=100,\n", "    lr=0.002,\n", "    optimizer=torch.optim.Adam,\n", "    callbacks=[cp, es],\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"jupyter": {"outputs_hidden": true}}, "outputs": [], "source": ["cnn.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(cnn.history[:, 'train_loss'], label='Train loss')\n", "plt.plot(cnn.history[:, 'valid_loss'], label='Validation loss')\n", "plt.xlabel('Epoch')\n", "plt.ylabel('Loss')\n", "plt.legend()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cnn.load_params(checkpoint=cp)\n", "\n", "y_pred = cnn.predict(X_val)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "print(classification_report(y_pred, y_val))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_probs = cnn.predict_proba(X_val)\n", "y_probs[:10]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import utils\n", "\n", "utils.visualize(X_val, y_val, y_probs,\n", "                ncols=5, nrows=3,\n", "                shape=(32, 32),\n", "                classes=encoder.classes_\n", "               )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## The kernels"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w1 = cnn.module_[0].weight.detach().numpy()\n", "w1.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(nrows=4, ncols=8, figsize=(12, 6))\n", "for w, ax in zip(w1, axs.ravel()):\n", "    ax.imshow(np.sum(w, axis=0))\n", "    ax.axis('off')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w2 = cnn.module_[3].weight.detach().numpy()\n", "\n", "fig, axs = plt.subplots(nrows=2, ncols=8, figsize=(12, 3))\n", "for w, ax in zip(w2, axs.ravel()):\n", "    ax.imshow(np.sum(w, axis=0))\n", "    ax.axis('off')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Using grid search with this network"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In theory, you can also use `scikit-learn` training flow objects to train robust models such as `GridSearchCV`.\n", "\n", "This is potentially cool, but I am not 100% certain that the models are initializing for each model. If you want to use this, I suggest reading the `skorch` docs carefully."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV\n", "\n", "cnn = NeuralNetClassifier(\n", "    net,\n", "    max_epochs=10,\n", "    batch_size=100,\n", "    optimizer=torch.optim.Adam,\n", ")\n", "\n", "params = {\n", "    'lr': [0.001, 0.003],\n", "}\n", "\n", "gs = GridSearchCV(cnn, params, refit=False, cv=3, scoring='accuracy')\n", "\n", "gs.fit(X_train, y_train)\n", "print(gs.best_score_, gs.best_params_)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model persistence\n", "\n", "The easiest way to save a model is to `pickle` the trained model object."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pickle\n", "\n", "# Saving.\n", "with open('torch_classifier.pkl', 'wb') as f:\n", "    pickle.dump(cnn, f)\n", "\n", "# Loading.\n", "with open('torch_classifier.pkl', 'rb') as f:\n", "    cnn = pickle.load(f)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cnn.initialize()\n", "cnn.predict(X_val)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "&copy; 2020 Agile Scientific"]}], "metadata": {"kernelspec": {"display_name": "geoml", "language": "python", "name": "geoml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}}, "nbformat": 4, "nbformat_minor": 4}