{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## A quick introduction to HDF5 files for seismic\n", "\n", "We will read a preformed dataset from a NumPy file, then save the dataset as an HDF5 file.\n", "\n", "This notebook follows on from `Intro_to_seismic.ipynb`.\n", "\n", "## What are HDF5 files?\n", "\n", "'HDF' stands for _hierarchical data format_. \n", "\n", "An HDF5 **File** can contain multiple **Group** and **Dataset** items. \n", "\n", "- A **Group** is a bit like a file system directory, and a bit like a Python dictionary. Groups can be arbitrarily nested (hence the _H_ in HDF). They can contain Datasets, or other Groups. The **File** is, for all intents and purposes, a root-level group.\n", "\n", "- A **Dataset** is a lot like a NumPy array. It's an n-dimensional (hyper-)rectangular data object, containing elements of homogenous type.\n", "\n", "Both Groups and Datasets can have **Attributes**: a dictionary-like `attrs` object attached to them, which holds metadata.\n", "\n", "HDF5 files support compression, error detection, metadata, and other useful things. They also support chunking, which can dramatically speed up data access on large files ([more about this](http://geology.beer/2015/02/10/hdf-for-large-arrays/)). "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["volume = np.load('../data/F3_volume_3x3_16bit.npy')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["volume.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Saving to HDF5\n", "\n", "We'll be using the HDF5 library, via [the `h5py` Python library](http://docs.h5py.org/en/stable/index.html).\n", "\n", "If you need to, you can install `h5py` from the Notebook with: `!conda install h5py`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import h5py"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The `h5py.File` object is a context manager, so we will use it that way. Let's write our seismic volume to an HDF5 file:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'w') as f:\n", "    dset = f.create_dataset(\"volume\", data=volume)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["That has saved a file:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%ls -l ../data/f3_seismic.hdf5"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is a bit bigger than the array:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["volume.nbytes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["So there's a little bit more overhead than a NumPy binary file:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%ls -l ../data/F3_volume_3x3_16bit.npy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can compress the dataset (there's not a lot you can do on this data, but it does squeeze down a bit) &mdash; this reduces the file size, but takes a little time."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'w') as f:\n", "    dset = f.create_dataset(\"volume\", data=volume, compression='gzip')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%ls -l ../data/f3_seismic.hdf5"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's add a bit of metadata:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'w') as f:\n", "    grp = f.create_group('amplitude_group')\n", "\n", "    dset = f.create_dataset(\"amplitude_group/timeseries_dataset\", data=volume)\n", "    \n", "    # Let's add some attributes (metadata) to the root-level File object...\n", "    f.attrs['survey'] = 'F3'\n", "    f.attrs['location'] = 'Netherlands'\n", "    f.attrs['owner'] = 'NAM/NLOG/TNO/dGB'\n", "    f.attrs['licence'] = 'CC-BY-SA'\n", "    \n", "    # ...and to the group...\n", "    grp.attrs['kind'] = 'Raw seismic amplitude, no units'\n", "    \n", "    # ...and to the dataset.\n", "    dset.attrs['domain'] = 'time'\n", "    dset.attrs['dt'] = '0.004'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Reading HDF5\n", "\n", "You will need to poke around a bt to figure out what the paths and datasets are. At first, it's best just to read the metadata, unless you know what you're expecting to find."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r') as f:\n", "    print([k for k in f.keys()])\n", "    print([k for k in f.attrs])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r') as f:\n", "    print([k for k in f['amplitude_group'].keys()])\n", "    print([k for k in f['amplitude_group'].attrs])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once you've figured out what you want, you can read the data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r') as f:\n", "    data = f['amplitude_group/timeseries_dataset'][:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["But we can also read slices from the volume without loading it all into memory:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r') as f:\n", "    data = f['amplitude_group/timeseries_dataset'][..., 200]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.imshow(data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Adding a new dataset to an HDF5 file\n", "\n", "Let's add a frequency cube with dimensions inlines, crosslines, frequency."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import scipy.signal\n", "\n", "freqs, Pxx_den = scipy.signal.welch(volume, fs=250)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.imshow(Pxx_den[:, :, 20])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Pxx_den.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r+') as f:\n", "\n", "    dset = f.create_dataset(\"amplitude_group/frequency_dataset\", data=Pxx_den)\n", "    \n", "    # This time we only want to add metadata to the dataset.\n", "    dset.attrs['domain'] = 'frequency'\n", "    dset.attrs['df'] = str(125 / 128)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The HDF5 file has grown commensurately:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%ls -l ../data/f3_seismic.hdf5"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r') as f:\n", "    print([k for k in f['amplitude_group'].keys()])\n", "    print([k for k in f['amplitude_group/frequency_dataset'].attrs])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r') as f:\n", "    print('domain =', f['amplitude_group/frequency_dataset'].attrs['domain'])\n", "    print('df =', f['amplitude_group/frequency_dataset'].attrs['df'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r') as f:\n", "    print(np.mean(f['amplitude_group/frequency_dataset']))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Modifying an existing dataset\n", "\n", "You can't point a name at a new dataset, but you can change the values inside a dataset &mdash; as long as it doen't change shape."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r+') as f:\n", "\n", "    data = f[\"amplitude_group/frequency_dataset\"]\n", "    data[...] = np.sqrt(Pxx_den)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r') as f:\n", "    print(np.mean(f['amplitude_group/frequency_dataset']))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If it changes shape, you'll have to delete it and add it again:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = np.mean(Pxx_den, axis=1)\n", "data.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r+') as f:\n", "\n", "    del(f[\"amplitude_group/frequency_dataset\"])\n", "\n", "    dset = f.create_dataset(\"amplitude_group/frequency_dataset\", data=data)\n", "    \n", "    # This time we only want to add metadata to the dataset.\n", "    dset.attrs['domain'] = 'frequency'\n", "    dset.attrs['df'] = str(125 / 128)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with h5py.File('../data/f3_seismic.hdf5', 'r') as f:\n", "    print(f['amplitude_group/frequency_dataset'].shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<hr />\n", "\n", "<div>\n", "<img src=\"https://avatars1.githubusercontent.com/u/1692321?s=50\"><p style=\"text-align:center\">\u00a9 Agile Geoscience 2018</p>\n", "</div>"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "geocomp", "language": "python", "name": "geocomp"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}}, "nbformat": 4, "nbformat_minor": 1}