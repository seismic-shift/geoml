{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Map interpolation\n", "\n", "Interpolation is a kind of regression (and, arguably, all prediction is interpolation... so all machine learning models are maps). And we have a bunch of different nonlinear regressors available to us in `scikit-learn`. So let's apply some of them to a mapping problem.\n", "\n", "We're going to start with a Gaussian process to model a surface. This is equivalent to kriging in the language of geostatistics. `sklearn` implements a sophisticated model, with stationary or non-stationary and isotropic or nonisotropic kernels \u2014\u00a0[read about it](https://scikit-learn.org/stable/modules/gaussian_process.html). \n", "\n", "[This tutorial](https://towardsdatascience.com/quick-start-to-gaussian-process-regression-36d838810319) is worth reading.\n", "\n", "I'm using a small dataset originally from Geoff Bohling at the Kansas Geological Survey. I can no longer find the data online. We will look at making a map using the Scikit-Learn machine learning library with `sklearn.gaussian_process`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Read the data"]}, {"cell_type": "markdown", "metadata": {"tags": ["exercise"]}, "source": ["<div style=\"background: #e0ffe0; border: solid 2px #d0f0d0; border-radius:3px; padding: 1em; color: darkgreen\">\n", "<h3>EXERCISE</h3>\n", "\n", "The data file is `../data/ZoneA.dat` (it's also on the web at `https://www.dropbox.com/s/6dyfc4fl5slhgry/ZoneA.dat?raw=1`)\n", "\n", "Load the data from the text file into a Pandas DataFrame. <a title=\"Use Pandas's read_csv() method. You will need to set some arguments: sep, header, usecols, names, and dtype.\"><b>Hover for HINT</b></a>\n", "\n", "We don't need all of the fields, but we at least need x, y, thickness and porosity. The top of the DataFrame should look something like this:\n", "\n", "              x       y     thick       por\n", "    0   12100.0  8300.0   37.1531   14.6515\n", "    1    5300.0  8700.0   31.4993   14.5093\n", "    \n", "Once you have the data, can you make a KDE plot (`sns.kdeplot()`) with Seaborn?", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# YOUR CODE HERE\n", "fname = \"../data/ZoneA.dat\"\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This next bit looks a bit unpleasant, but we're just getting out min and max values for the x and y columns."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["extent = x_min, x_max, y_min, y_max = [df.x.min()-1000, df.x.max()+1000,\n", "                                       df.y.min()-1000, df.y.max()+1000]"]}, {"cell_type": "markdown", "metadata": {"tags": ["exercise"]}, "source": ["<div style=\"background: #e0ffe0; border: solid 2px #d0f0d0; border-radius:3px; padding: 1em; color: darkgreen\">\n", "<h3>EXERCISE</h3>\n", "\n", "Make a map of the porosity data. Use the colour of the points to indicate porosity, and use the size to indicate thickness of the zone.\n", "\n", "Add a colourbar, title, and other annotation.", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# YOUR CODE HERE\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Make a grid\n", "\n", "Now we can make a regular grid:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Notice the order of x (columns) and y (rows)!\n", "grid_y, grid_x = np.mgrid[y_min:y_max:500, x_min:x_max:500]\n", "\n", "# Use *shape* argument to specify the *number* (not size) of bins:\n", "# grid_y, grid_x = np.mgrid[y_min:y_max:100j, x_min:x_max:100j]\n", "\n", "plt.figure(figsize=(10, 6))\n", "plt.scatter(grid_x, grid_y, marker='+', lw=0.5, s=10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Fit a model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.gaussian_process.kernels import RBF\n", "\n", "kernel = RBF(length_scale=1000)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.gaussian_process import GaussianProcessRegressor\n", "\n", "gp = GaussianProcessRegressor(normalize_y=True,\n", "                              alpha=0.5,  # Larger values imply more noise in the input data.\n", "                              kernel=kernel,)\n", "\n", "gp.fit(df[['x', 'y']].values, df.por.values)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To make a prediction, we need to construct the `X` matrix: (x, y) coordinates in 2 columns:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_grid = np.stack([grid_x.ravel(), grid_y.ravel()]).T"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can make a prediction:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_grid = gp.predict(X_grid).reshape(grid_x.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And plot the predicted grid with the input data using the same colourmap:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute min and max of all the data:\n", "mi = np.min(np.hstack([y_grid.ravel(), df.por.values]))\n", "ma = np.max(np.hstack([y_grid.ravel(), df.por.values]))\n", "\n", "# Plot it all.\n", "plt.figure(figsize=(15, 15))\n", "im = plt.imshow(y_grid, origin='lower', extent=extent, vmin=mi, vmax=ma)\n", "pts = plt.scatter(df.x, df.y, c=df.por, s=80, edgecolor='#ffffff66', vmin=mi, vmax=ma)\n", "plt.colorbar(im, shrink=0.67)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Evaluate the model\n", "\n", "We can compute the residual error distribution by making a prediction on the original (x, y) values and comparing to the actual measured porosities at those locations:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["por_hat = gp.predict(df[['x', 'y']].values)\n", "\n", "sns.kdeplot(por_hat - df.por)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also compute the RMS error:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import mean_squared_error\n", "\n", "rmse = np.sqrt(mean_squared_error(df.por, por_hat))\n", "\n", "print(f\"RMS error: {rmse:.3f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Going further\n", "\n", "The GP model actually models the distribution, not just the mean (which for a normal distribution is also the most likely or mode, and the P50 or median).\n", "\n", "So we can also look at samples from the distribution:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_sample = gp.sample_y(X_grid, n_samples=3, random_state=42)\n", "\n", "fig, axs = plt.subplots(ncols=3, figsize=(15, 5))\n", "for ax, y_hat in zip(axs, y_sample.T):\n", "    im = ax.imshow(y_hat.reshape(grid_x.shape), origin='lower', extent=extent)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's look at a cross-section too.\n", "\n", "We'll want the 'wells' on there at some point, so let's add grid-cell coordinates. (A better way to do this would be to use `xarray`.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df['x_grid'] = (df.x - x_min) // 500\n", "df['y_grid'] = (df.y - y_min) // 500"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can get the mean and stdev of the prediction:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_grid, y_std = gp.predict(X_grid, return_std=True)\n", "\n", "y_grid = y_grid.reshape(grid_x.shape)\n", "y_std = y_std.reshape(grid_x.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then choose a row to look at, and filter the wells down to near that row:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["row = 19\n", "wells = df[abs(df.y_grid - row) <= 1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's check where that row and its wells are:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 10))\n", "im = plt.imshow(y_grid, origin='lower', extent=extent, vmin=mi, vmax=ma)\n", "pts = plt.scatter(df.x, df.y, c=df.por, s=80, edgecolor='#ffffff66', vmin=mi, vmax=ma)\n", "pts = plt.scatter(wells.x, wells.y, c=wells.por, s=80, edgecolor='r', vmin=mi, vmax=ma)\n", "plt.colorbar(im, shrink=0.67)\n", "plt.axhline(y_min + 19 * 500, c='red')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And compute the various things:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Confidence interval.\n", "phi, s = y_grid[row], y_std[row]\n", "max_ = phi - 1.96 * s  # 95% confidence interval\n", "min_ = phi + 1.96 * s  #     = 1.96 * stdev\n", "\n", "# Draw some samples.\n", "y_samples = gp.sample_y(X_grid, n_samples=5, random_state=42)\n", "\n", "# Plot.\n", "plt.figure(figsize=(15, 5))\n", "for i, well in wells.iterrows():\n", "    plt.scatter(well.x_grid, well.por, c='none', ec='r', s=80, zorder=100)\n", "    plt.axvline(well.x_grid, c='k', alpha=0.15)\n", "plt.plot(phi, c='darkblue', lw=4, alpha=0.67)\n", "for sample in y_samples.T:\n", "    plt.plot(sample.reshape(grid_x.shape)[row])\n", "plt.fill_between(range(phi.size), min_, max_, alpha=0.2)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## More models"]}, {"cell_type": "markdown", "metadata": {"tags": ["exercise"]}, "source": ["<div style=\"background: #e0ffe0; border: solid 2px #d0f0d0; border-radius:3px; padding: 1em; color: darkgreen\">\n", "<h3>EXERCISE</h3>\n", "\n", "Try some other regressors in `scikit-learn`. Look at the maps they produce, and check their residuals. (You might want to write a loop to check several models.)\n", "\n", "Which regressors produce the best maps? ", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# YOUR CODE HERE\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you're interested in this sort of thing, check out\n", "\n", "- [This blog post](http://geologyandpython.com/ml-interpolation-method.html) on the subject.\n", "- And [this paper](https://arxiv.org/pdf/2006.10461.pdf) on a clever CNN-based approach that incorporates Moran's I embedding."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Superpowers with `verde` and `gstools`\n", "\n", "The geophysical package `verde` contains still more gridding algorithms, and makes some aspects of the gridding much easier \u2014\u00a0for example reducing the nuber of points in the dataset in sensible ways. [Read about it.](https://www.leouieda.com/blog/introducing-verde.html) Better yet, [try it out.](https://github.com/agile-geoscience/xlines/blob/master/notebooks/11_Gridding_map_data.ipynb)\n", "\n", "The geostatistical package `gstools` implements various kriging algorithms, which some people may be more familiar with. [Check it out](https://github.com/GeoStat-Framework/GSTools). "]}, {"cell_type": "markdown", "metadata": {}, "source": ["----\n", "\n", "<center>&copy; 2020 Agile Scientific, licensed CC-BY</center>"]}], "metadata": {"kernelspec": {"display_name": "geoml", "language": "python", "name": "geoml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.10"}}, "nbformat": 4, "nbformat_minor": 2}