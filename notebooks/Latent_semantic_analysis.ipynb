{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### UNSUPERVISED LEARNING\n", "\n", "# Recommending documents with LSA\n", "\n", "----\n", "\n", "#### \u2757 NLTK is hard to install. I recommend running this notebook in Google Colab instead: https://drive.google.com/file/d/1xel4VmTqzFoZkOiEijyGhYuH6BQW5lYM/view?usp=sharing\n", "\n", "----\n", "\n", "We'd like to find documents with similar content to a document we like, but without having to rely on tagging or other labels. This is what **latent semantic analysis** is for. We can 'sense' the meaning of a document from the words it contains.\n", "\n", "Inspired by and/or based on [**science concierge**](https://github.com/titipata/science_concierge) and [**Chris Clark's repo**](https://github.com/groveco/content-engine) on content-based recommendation.\n", "\n", "[This blog post](https://www.themarketingtechnologist.co/a-recommendation-system-for-blogs-content-based-similarity-part-2/) is also really good. [Pysuggest](https://pypi.python.org/pypi/pysuggest) might be worth looking at, and so might [Crab](https://muricoca.github.io/crab/).\n", "\n", "Believe it or not, we can do all of it in about 10 lines of code!\n", "\n", "----\n", "\n", "We'll start with some data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "df = pd.read_csv('https://raw.githubusercontent.com/seg/2017-tle-hall/master/data/title_abstract_doi.csv')\n", "\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Prepare the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from nltk.stem.porter import PorterStemmer\n", "from nltk.tokenize import RegexpTokenizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Instantiate the stemmer and tokenizer.\n", "stemmer, tokenizer = PorterStemmer(), RegexpTokenizer(r'\\w+')\n", "\n", "# Make a function to preprocess each item in the data.\n", "def preprocess(item):  # 3\n", "    return ' '.join(stemmer.stem(token) for token in tokenizer.tokenize(item))\n", "\n", "# Apply the preprocessing.\n", "data = [preprocess(item) for item in df.abstract]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Compute the document matrix\n", "\n", "The matrix is a **term frequency, inverse document frequency** or \"tfidf\" matrix. This counts how many times words and/or phrases ('terms') appear in a document, then scales those frequencies to the inverse of how frequent they are in the cohort. So a rare word like 'coulomb' carries more weight than a common one like 'seismic'.\n", "\n", "The `sklearn` implementation automatically filters 'stop' words, eliminating things like 'the' or 'this'. It works just like `sklearn`'s other models:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import TfidfVectorizer\n", "\n", "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,1))\n", "vecs = tfidf.fit_transform(data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The resulting matrix has one row for each document, and one colun for each 'term'. If we include n-grams, which are groups of words, the matrix will be very large."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vecs.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Reduce the number of dimensions\n", "\n", "To make the matrix more manageable, we can reduce the number of dimensions with singular value decomposition. We'll reduce it down to 100 dimensions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.decomposition import TruncatedSVD\n", "\n", "svd = TruncatedSVD(n_components=100).fit_transform(vecs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Build and store the distance tree\n", "\n", "The distance tree is a fast dta structure for finding nearest neighbours in a high-dimensional space."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.neighbors import KDTree\n", "\n", "tree = KDTree(svd)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Query the tree for recommendations\n", "\n", "Now we can find a paper we're interested in and try to find similar papers."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["target = 333\n", "\n", "df.title[target]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Recommend 5 docs for a single document.\n", "_, idx = tree.query([svd[target]], k=6)\n", "\n", "[df.title[i] for i in idx[0] if i != target]"]}, {"cell_type": "markdown", "metadata": {"tags": ["exercise"]}, "source": ["<div style=\"background: #e0ffe0; border: solid 2px #d0f0d0; border-radius:3px; padding: 1em; color: darkgreen\">\n", "<h3>Exercise</h3>\n", "\n", "- Can you visualize the document clusters with t-SNE or UMAP?\n", "\n", "See the **Unsupervised clustering** notebook.", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "geoml", "language": "python", "name": "geoml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}}, "nbformat": 4, "nbformat_minor": 1}