{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Semantic segmentation with U-Net\n", "\n", "- U-Net is an architecture developed to generate mapping between images and correspoding masks for each image.\n", "- It was developed for 'few shot' learning: making a reliable model with few training examples.\n", "- Original U-net paper (https://arxiv.org/abs/1505.04597)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# About the data\n", "\n", "The seismic data used in this exercise is from the Kerry 3D data set offshore New Zealand: https://dataunderground.org/dataset/kerry. You do not need to download the entire dataset for this exercise. The training images have already been created for you.\n", "\n", "The training data consists of a series of subsections (tiles) along inlines across the volume. The modeled fault regions where picked using Microsoft's Visual Object Tagging Tool, https://github.com/microsoft/VoTT. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["First we'll import the usual supporting libraries."]}, {"cell_type": "code", "execution_count": null, "metadata": {"inputHidden": false, "outputHidden": false}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from PIL import Image"]}, {"cell_type": "code", "execution_count": null, "metadata": {"inputHidden": false, "outputHidden": false}, "outputs": [], "source": ["import tensorflow as tf\n", "from tensorflow.keras.models import Model, load_model\n", "from tensorflow.keras.layers import (Input, BatchNormalization, Activation, Dense, Dropout,\n", "                                    Lambda, RepeatVector, Reshape, Conv2D, Conv2DTranspose,\n", "                                    MaxPooling2D, GlobalMaxPool2D, UpSampling2D, concatenate, add)\n", "\n", "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n", "from tensorflow.keras.optimizers import Adam\n", "from tensorflow.keras.preprocessing.image import (array_to_img, img_to_array,\n", "                                                 load_img)\n", "\n", "from tensorflow.keras.preprocessing.image import ImageDataGenerator"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cross_entropy_balanced(y_true, y_pred):\n", "    # Implementation from https://github.com/xinwucwp/faultSeg\n", "    # Note: tf.nn.sigmoid_cross_entropy_with_logits expects y_pred is logits, \n", "    # Keras expects probabilities.\n", "    # transform y_pred back to logits\n", "    _epsilon = _to_tensor(tf.keras.backend.epsilon(), y_pred.dtype.base_dtype)\n", "    y_pred   = tf.clip_by_value(y_pred, _epsilon, 1 - _epsilon)\n", "    y_pred   = tf.math.log(y_pred/ (1 - y_pred))\n", "\n", "    y_true = tf.cast(y_true, tf.float32)\n", "\n", "    count_neg = tf.reduce_sum(1. - y_true)\n", "    count_pos = tf.reduce_sum(y_true)\n", "\n", "    beta = count_neg / (count_neg + count_pos)\n", "\n", "    pos_weight = beta / (1 - beta)\n", "\n", "    cost = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, labels=y_true, pos_weight=pos_weight)\n", "\n", "    cost = tf.reduce_mean(cost * (1 - beta))\n", "\n", "    return tf.where(tf.equal(count_pos, 0.0), 0.0, cost)\n", "\n", "\n", "def _to_tensor(x, dtype):\n", "    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n", "    # Arguments\n", "    x: An object to be converted (numpy array, list, tensors).\n", "    dtype: The destination type.\n", "    # Returns\n", "    A tensor.\n", "    \"\"\"\n", "    x = tf.convert_to_tensor(x)\n", "    if x.dtype != dtype:\n", "        x = tf.cast(x, dtype)\n", "    return x\n", "\n", "def little_unet(input_size=(256,256,1)):\n", "    inputs = Input(input_size)\n", "    conv1 = Conv2D(16, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(inputs)\n", "    conv1 = Conv2D(16, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(conv1)\n", "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n", "\n", "    conv2 = Conv2D(32, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(pool1)\n", "    conv2 = Conv2D(32, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(conv2)\n", "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n", "\n", "    conv3 = Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(pool2)\n", "    conv3 = Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(conv3)\n", "    pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n", "\n", "    conv4 = Conv2D(512, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(pool3)\n", "    conv4 = Conv2D(512, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(conv4)\n", "\n", "    up5 = concatenate([UpSampling2D(size=(2,2))(conv4), conv3], axis=3)\n", "    conv5 = Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(up5)\n", "    conv5 = Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(conv5)\n", "\n", "    up6 = concatenate([UpSampling2D(size=(2,2))(conv5), conv2], axis=3)\n", "    conv6 = Conv2D(32, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(up6)\n", "    conv6 = Conv2D(32, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(conv6)\n", "\n", "    up7 = concatenate([UpSampling2D(size=(2,2))(conv6), conv1], axis=3)\n", "    conv7 = Conv2D(16, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(up7)\n", "    conv7 = Conv2D(16, (3,3), activation='relu', padding='same', kernel_initializer = 'he_normal')(conv7)\n", "\n", "    conv8 = Conv2D(1, (1,1), activation='sigmoid')(conv7)\n", "\n", "    model = Model(inputs, conv8)\n", "\n", "    model.compile(optimizer = Adam(lr = 1e-4), loss = cross_entropy_balanced, metrics = ['accuracy'])\n", "\n", "    return model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To generate more samples from the training dataset we have, we can use an ImageDataGenerator object. This object can be parametrized to produce more samples for training automatically."]}, {"cell_type": "code", "execution_count": null, "metadata": {"inputHidden": false, "outputHidden": false}, "outputs": [], "source": ["aug_args = dict(rotation_range=0.2,\n", "                width_shift_range=0.05,\n", "                height_shift_range=0.05,\n", "                shear_range=0.05,\n", "                zoom_range=0.05,\n", "                horizontal_flip=True,\n", "                fill_mode='nearest')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def scaleData(img, mask):\n", "    \"\"\"Helper function to normalize images and masks.\n", "    It also binarizes the mask.\n", "    \"\"\"\n", "    img = img / 255\n", "    mask = mask > 127\n", "    return img, mask\n", "\n", "def trainGenerator(train_path, image_folder, mask_folder, aug_dict, seed = 1):\n", "    \"\"\"Produces samples of training images from the train pool of images.\n", "    I can generate infinite samples based on the augmentation parameters.\n", "    \"\"\"\n", "\n", "    image_datagen = ImageDataGenerator(**aug_dict)\n", "    mask_datagen = ImageDataGenerator(**aug_dict)\n", "    \n", "    image_generator = image_datagen.flow_from_directory(\n", "        train_path,\n", "        classes = [image_folder],\n", "        class_mode = None,\n", "        color_mode = \"grayscale\",\n", "        target_size = (256,256),\n", "        batch_size = 2,\n", "        seed = seed)\n", "    \n", "    mask_generator = mask_datagen.flow_from_directory(\n", "        train_path,\n", "        classes = [mask_folder],\n", "        class_mode = None,\n", "        color_mode = \"grayscale\",\n", "        target_size = (256,256),\n", "        batch_size = 2,\n", "        seed = seed)\n", "    \n", "    train_generator = zip(image_generator, mask_generator)\n", "    \n", "    for img, mask in train_generator:\n", "        img, mask = scaleData(img, mask)\n", "        yield img, mask"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We initialize and store the data generation object that will feed the network. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_gen = trainGenerator('../data/faultpick/', 'seismic', 'fault', aug_args)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialize the network."]}, {"cell_type": "code", "execution_count": null, "metadata": {"inputHidden": false, "outputHidden": false}, "outputs": [], "source": ["model = little_unet()\n", "model_checkpoint = ModelCheckpoint('seis_unet.hdf5', monitor='loss', \n", "                                   verbose=1, save_best_only=True,\n", "                                   save_weights_only=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train the network."]}, {"cell_type": "code", "execution_count": null, "metadata": {"inputHidden": false, "outputHidden": false}, "outputs": [], "source": ["model.fit(data_gen, steps_per_epoch=50, epochs=5, callbacks=[model_checkpoint])"]}, {"cell_type": "markdown", "metadata": {"tags": ["exe"]}, "source": ["<div style=\"background: #e0ffe0; border: solid 2px #d0f0d0; border-radius:3px; padding: 1em; color: darkgreen\">\n", "<h3>EXERCISE</h3>\n", "\n", "Write a workflow to make predictions from the images in `'../data/faultpick/test/'` using the model trained.", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["img_arr = np.array(img, dtype=float)[:,:,0]\n", "img_arr /= 255.0\n", "\n", "plt.imshow(img_arr)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernel_info": {"name": "geocomp-ml-gpu"}, "kernelspec": {"display_name": "geoml", "language": "python", "name": "geoml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}, "nteract": {"version": "0.15.0"}}, "nbformat": 4, "nbformat_minor": 4}