{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Gradient descent to solve linear regression\n", "\n", "We'll implement stochastic gradient descent from scratch, and use it to solve an ordinary least-squares linear regression.\n", "\n", "Caveat: we don't actually use gradient descent, or the normal equation, to solve this problem. [Read why](https://towardsdatascience.com/why-gradient-descent-and-normal-equation-are-bad-for-linear-regression-928f8b32fa4f). The purpose of this notebook is to look at how gradient descent works.\n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, let's look at what **gradient descent** is trying to do.\n", "\n", "The basic idea is that we have some function we'd like to minimize. If you think of the function as a physical surface, we're going to try to walk (or ski!) down it to a minimum, hopefully the global minimum. \n", "\n", "The function we're trying to minimize is the **cost function**. This is whatever we want it to be, but in our example it's going to be the mean squared error... hence the name 'least squares solution'.\n", "\n", "If we think of the cost function as a function of the **parameters** (weights, or coefficients), then we can use the partial derivative of the function with respect to each parameter to adjust that weight... although actually it turns out we can avoid updating the parameters separately by treating them as a single vector.\n", "\n", "<img src=\"../images/gradient_descent.png\" width=\"800px\" />"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load some data\n", "\n", "We'll use our 'rocks' data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "df = pd.read_csv('../data/rocks.csv')\n", "\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's try to predict `Vs` from `Vp` and `Rho_n`: "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = df[['Vp', 'Rho_n', 'Vs']].values"]}, {"cell_type": "markdown", "metadata": {"tags": ["advanced"]}, "source": ["<div style=\"background: #fff0e0; border: solid 2px #ffe7d0; border-radius:3px; padding: 1em; color: chocolate\">\n", "<h3>Random data</h3>\n", "\n", "<p>If you prefer, you can make a dataset from a multivariate normal distribution, so we can control the statistics of the features and target.</p>\n", "\n", "<b>To do this, run the following code:</b>\n", "\n", "<pre>\n", "import numpy as np\n", "\n", "num_samples = 100\n", "\n", "<h3>The desired mean values of the sample.</h3>\n", "mu = np.array([5.0, 5.0, 10.0])\n", "\n", "<h3>The desired covariance matrix.</h3>\n", "r = np.array([\n", "        [  2.00,  0.00, -2.00],\n", "        [  0.00,  2.00,  2.00],\n", "        [ -2.00,  2.00,  4.00]\n", "    ])\n", "\n", "<h3>Generate the random samples.</h3>\n", "data = np.random.multivariate_normal(mu, r, size=num_samples)\n", "</pre>", "\n</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Make `X` and `y`"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can visualize the data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import seaborn as sns\n", "import pandas as pd\n", "\n", "df = pd.DataFrame(data, columns=['x0', 'x1', 'y'])\n", "\n", "sns.pairplot(df, )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = data[:, 0:2]\n", "y = data[:, 2]\n", "\n", "print(X.shape, y.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n", "\n", "X = StandardScaler().fit_transform(X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Solve with `sklearn`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "from sklearn.metrics import mean_squared_error\n", "\n", "model = LinearRegression().fit(X, y)\n", "\n", "y_pred = model.predict(X)\n", "print(f\"MSE: {mean_squared_error(y, y_pred):.3f}\\n\")\n", "\n", "(w0, w1), b = model.coef_, model.intercept_\n", "print(f\"w0 = {w0:.3f}\\nw1 = {w1:.3f}\\nb = {b:.3f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "X_plt = np.arange(-3, 3, 0.1)\n", "y_plt_sk = model.coef_[0]*X_plt + model.coef_[1]*0 + model.intercept_\n", "\n", "plt.scatter(X[:, 0], y) \n", "plt.plot(X_plt, y_plt_sk, color='r')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Closed form solution\n", "\n", "There's actually a linear algebraic way to solve this equation. In principle, this is much faster and easier to do than gradient descent. In practice, it might not be possible, e.g. on very large datasets, or with custom loss functions.\n", "\n", "One way to solve the problem is with ordinary least-squares and the so-called [**normal equation**](https://en.wikipedia.org/wiki/Linear_least_squares#Derivation_of_the_normal_equations):\n", "\n", "$$ \\mathbf{y} = \\mathbf{w}\\mathbf{X} $$\n", "\n", "We want the weights $\\mathbf{w}$, so:\n", "\n", "$$ \\mathbf{w} = \\mathbf{X}^{-1}\\mathbf{y} $$\n", "\n", "The catch is that $\\mathbf{X}^{-1}$ may not be possible to compute, so we have to use [the pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of `X`.\n", "\n", "This approach is easier if we add a column of 1's to `X`, and concatenate the weights `w` with the bias `b`, so that we have a single data matrix and a single weight vector:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["N = X.shape[0]\n", "X_ = np.c_[X, np.ones((N, 1))]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["That [`np.c_`](https://numpy.org/doc/stable/reference/generated/numpy.c_.html) implements a concatenation trick. You can also use `np.hstack`.\n", "\n", "Now we can implement the 'naive' version of the normal equation:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.set_printoptions(suppress=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy.linalg as la\n", "\n", "X_inv = la.inv(X_.T @ X_) @ X_.T  # The 'pseudoinverse' of X.\n", "\n", "X_inv @ y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We solved it! Pretty cool. But it won't always work..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Gradient descent\n", "\n", "Let's solve the problem using gradient descent.\n", "\n", "In order to optimize the weights using gradient descent, we need to know the **gradient** of the **cost function**. The cost $C$ over $N$ samples is given by:\n", "\n", "$$ C = \\frac{1}{N} \\sum_{i=0}^{N} (\\mathbf{y}_i - \\hat{\\mathbf{y}}_i)^2 $$\n", "\n", "where the estimate $\\hat{\\mathbf{y}}$ is given by the matrix product of the weights $\\theta$ and the 'augmented' data $\\bar{\\mathbf{X}}$ (the data plus an extra column of ones):\n", "\n", "$$ \\hat{\\mathbf{y}} = \\mathbf{w} \\mathbf{X} + b = \\theta \\bar{\\mathbf{X}} $$\n", "\n", "Using this 'combined' weight matrix is easier than dealing with the weights and bias term separately. Note that $\\theta$ is a vector, although it's displayed with an italic character here.\n", "\n", "#### Chain rule\n", "\n", "There are 2 functions here: the expression for $C$ and the one for $\\hat{\\mathbf{y}}$. The chain rule tells us how to differentiate a function of a function:\n", "\n", "$$ \\frac{\\mathrm{d}}{\\mathrm{d}x}[f(g(x))] = f^\\prime(g(x))\\ \\times\\ g^\\prime(x) $$\n", "\n", "Therefore the gradient $g$ is given by:\n", "\n", "$$ g = \\frac{\\mathrm{d}C}{\\mathrm{d}\\theta} = \\frac{\\mathrm{d}C}{\\mathrm{d}\\hat{\\mathbf{y}}}\\ \\times\\ \\frac{\\mathrm{d}\\hat{\\mathbf{y}}}{\\mathrm{d}\\theta} $$\n", "\n", "So our problem now looks like this:\n", "\n", "$$ g = \\frac{2}{N}\\sum_{i=0}^{N} (\\mathbf{y}_i - \\hat{\\mathbf{y}}_i)\\ \\times\\ -\\mathbf{X}_i $$\n", "\n", "so if the error $\\mathbf{e} = (\\mathbf{y} - \\hat{\\mathbf{y}})$ then:\n", "\n", "$$ g = -\\frac{2}{N}\\sum_{i=0}^{N} \\mathbf{e}_i \\mathbf{X}_i $$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alpha = 0.001\n", "epochs = 5_000\n", "\n", "# Initialize.\n", "cost_history = []\n", "theta = np.zeros(3)\n", "\n", "for i in range(epochs):\n", "    # Make estimate with current weights.\n", "    y_hat = theta @ X_.T\n", "    \n", "    # Compute error and cost.\n", "    err = y - y_hat\n", "    cost = np.sum(err**2) / N\n", "    cost_history.append(cost)\n", "    if i % (epochs // 10) == 0:\n", "        print(f\"{i:>8d} {cost:.3f}\")\n", "        \n", "    # Compute gradients (one for each feature).\n", "    grad = (-2/N) * np.sum(err[:, None] * X_, axis=0)\n", "    \n", "    # Adjust weights.\n", "    theta -= alpha * grad\n", "\n", "print(f\"{'end':>8s} {cost:.3f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(cost_history)\n", "plt.yscale('log')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's make sure it's similar to what we got before, which was"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# With sklearn...\n", "w0, w1, b"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# With gradient descent...\n", "theta"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Stochastic gradient descent\n", "\n", "Here, we're computing the cost over the entire dataset. In practice, this might not be possible, especially on high-dimensional datasets. We can reduce the computational burden by computing the cost over smaller 'batches'. This is known as 'stochastic gradient descent'. \n", "\n", "We'll also shuffle the data before each epoch (one epoch is one pass through the entire dataset). This helps to avoid getting stuck in a local minimum, because every time we take a step on the cost surface, we're on a slightly different version of the surface."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alpha = 0.001\n", "epochs = 1000\n", "batch_size = 50\n", "\n", "splits = np.arange(batch_size, N, batch_size)\n", "idx = np.arange(N)\n", "\n", "# Initialize.\n", "cost_history = []\n", "theta = np.random.random(3)\n", "\n", "for i in range(epochs):\n", "    \n", "    # Shuffle the data.\n", "    np.random.shuffle(idx)\n", "    X_shuffle = X_[idx]\n", "    y_shuffle = y[idx]\n", "    X_batches = np.split(X_shuffle, splits)\n", "    y_batches = np.split(y_shuffle, splits)\n", "\n", "    for j, (X_batch, y_batch) in enumerate(zip(X_batches, y_batches)):\n", "\n", "        # Make estimate with current weights.\n", "        y_hat = theta @ X_batch.T\n", "\n", "        # Compute error and (once per epoch) cost.\n", "        err = y_batch - y_hat\n", "        \n", "        if j == 0:\n", "            cost = np.sum(err**2) / batch_size\n", "            cost_history.append(cost)\n", "\n", "        # Compute gradients (one for each feature).\n", "        grad = (-2/batch_size) * np.sum(err[:, None] * X_batch, axis=0)\n", "\n", "        # Adjust weights.\n", "        theta -= alpha * grad\n", "\n", "    if i % (epochs // 10) == 0:\n", "        print(f\"{i:>8d} {cost:.3f}\")\n", "\n", "print(f\"{'end':>8s} {cost:.3f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(cost_history)\n", "plt.yscale('log')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice that this converges much faster than using the entire dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["theta"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This compares well with what we got from `sklearn`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w0, w1, b"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And let's check the new parameters against the previous solution:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w0_sgd, w1_sgd, b_sgd = theta\n", "\n", "y_plt = w0_sgd*X_plt + w1_sgd*0 + b_sgd\n", "\n", "plt.scatter(X[:, 0], y) \n", "plt.plot(X_plt, y_plt_sk, color='r')  # regression line\n", "plt.plot(X_plt, y_plt, '--', color='c')  # regression line\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It matches!\n", "\n", "----\n", "\n", "&copy; 2020 Agile Scientific"]}], "metadata": {"kernelspec": {"display_name": "geoml", "language": "python", "name": "geoml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.10"}}, "nbformat": 4, "nbformat_minor": 4}